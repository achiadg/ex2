---
title: "prediction2"
output: html_document
---

#R markdown for prediction1
##setup
```{r setup}
knitr::opts_knit$set(root.dir = 'C:/Users/אחיעד/Desktop/data science/ass2/prediction2')
getwd()
```

##read train.csv into dataframe
```{r}
df <- read.csv("C:/Users/אחיעד/Desktop/data science/ass2/Titanic/train.csv",na.strings = "")
```

##using str to identify data types
```{r}
str(df)
```

```{r}
df$Survived<- as.factor(df$Survived)
df$Pclass<- as.factor(df$Pclass)
```

##data summary
```{r}
summary(df)
```
##remove features from df

```{r}
df <- df[,-c(1,4,9)]
```

```{r}
cols<- 1:dim(df)[2]
factors <- cols[sapply(df,is.factor)]
numerics <- cols[!sapply(df,is.factor)]
```

We now tide the data two times: the first is for categorial data and the second for numeric data.

```{r}
#install.packages("tidyr")
library(tidyr)
df_tidy_factors<-gather(df[,factors],"feature","value", -1)
df_tidy_numerics<-gather(cbind(Survived=df[,1],df[,numerics]),"feature","value",-1)

```


Finally, we can plot. The first plot describes only categorical features (factors). 
Notice that the *scales* parameter was set to "free" to enable a suitable scaling for each facet (otherwise it is hard to view some of the facets, that need much smaller scales). We use the *facet_grid* that accepts a *scales* parameter.

```{r}
#install.packages("ggplot2")
library(ggplot2)
qplot(x=value,data=df_tidy_factors,fill=Survived) + facet_grid(~feature,scales="free")
```


One more plot for numeric features:
```{r}
qplot(x=value,data=df_tidy_numerics,fill=Survived) + facet_grid(~feature,scales="free")
```

```{r}
set.seed(50)
#indices <- sample(1:nrow(df),nrow(df)*0.75)
#train<- df[indices,]
#test<- df[-indices,]
df_ <- as.data.frame(na.omit(df))
df_1=data.matrix(df_,rownames.force = T)
head(df_1)
library(class)
library(caret)
control <- trainControl(method="cv", number=10)
model_knn<-train(df_1[,-1],df_1[,1],method='knn',tuneLength=20, na.action = na.pass)
"indices <- sample(1:nrow(df_1),nrow(df_1)*0.7)
train<- df_1[indices,]
test<- df_1[-indices,]"
#install.packages('class')

"pred <- knn(train, test, cl=as.factor(train[,'Survived']), k = 7)
pred
conf_tab<- table(test[,'Survived'],pred)
conf_tab
mean(pred==test[,'Survived'])"
```

It certainly looks luck there are more chances to survive in certain levels of almost each feature.

Train the model based on the train set:

```{r}
#install.packages('e1071')
#load e1071 library and invoke naiveBayes method
library(e1071)
nb_model <- naiveBayes(Survived~.,data = train)
nb_model
```

Predict target attribute for the test set based on the trained model:

```{r}
nb_test_predict <- predict(nb_model,test[,-1] ,na.action = na.pass)

```

Use a confusion matrix to evaluate the prediction results
```{r}
table(pred=nb_test_predict,true=test$Survived)

```


12. Calculate the model's test accuracy.

```{r}
mean(nb_test_predict==test$Survived)
```

14. Load the test file. Don't forget the empty strings issue when loading the file (na.strings = "").

```{r}
new_df <-read.csv('C:/Users/אחיעד/Desktop/data science/ass2/Titanic/test.csv',na.strings = "")
```


15. Create a vector with the PassengerIds of records in the test file. We will soon attach it to the prediction results.

```{r}
ids<- new_df$PassengerId
```

16. Repeat the same preprocessing steps that were performed on the train data (factorizing two features and ignoring three features). You must make sure that the train and test data have the same structure (except for the target feature)

```{r}
new_df$Pclass<- as.factor(new_df$Pclass)
new_df<- new_df[,-c(1,3,8)]
```


17. Another thing that is going to make troubles is that the test data contains some new levels in the *Cabin* feature that did not appear in the train data (you can check it by applying *Summary* on the test data). Add these new levels to the model.

- Getting the levels of the "Cabin" feature from a model *m*: m$xlevels[["Cabin"]]

- Getting the levels of the "Cabin" feature from a dataframe *d*: levels(d$Cabin)

- Union operation: union(x,y)


```{r}
#nb_model$xlevels[["Cabin"]] <- union(nb_model$xlevels[["Cabin"]], levels(new_df$Cabin))
model_knn$xlevels[["Cabin"]] <- union(model_knn$xlevels[["Cabin"]], levels(new_df$Cabin))
```


18. Load the test file and predict the target attribute for its records using your trained model. Don't forget to set *na.action* to "na.pass" if you used caret in the model creation, otherwise it will fail.

```{r}
new_df_no_na <- as.data.frame(na.omit(new_df))
new_df_no_na_num=data.matrix(new_df_no_na,rownames.force = T)
new_pred_knn<- predict(model_knn,new_df_no_na_num)
#new_pred_nb_model<- predict(nb_model,new_df,na.action = na.pass)
```

19. Write the *PassengerId* and *Survived* attributes to a csv file and submit this file to kaggle's competition (You should first subscribe to the site). What is your score?

```{r}
res <- cbind(PassengerId=ids,Survived=round(new_pred_knn)-1)
write.csv(res,file="C:/Users/אחיעד/Desktop/data science/ass2/prediction2/output.csv",row.names = F)
```



